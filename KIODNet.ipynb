{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><b>KIODNet: Adaptive Multivariate Time Series Classification Framework using Essembled Learning Approach for Indoor Outdoor Detection</center></h1></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>  Author:</h2></b>\n",
    "\n",
    "        Bilal Dastagir - ISI LAB, School of Computing, KAIST, Daejeon, South Korea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bilz/air/Datasets/trainingTestingData.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    dataset_Path = '/content/gdrive/MyDrive/Datasets/'\n",
    "    model_Path = '/content/gdrive/MyDrive/models/'\n",
    "except:\n",
    "    dataset_Path = '/home/bilz/air/Datasets/'\n",
    "    model_Path = '/home/bilz/air/models/'\n",
    "    \n",
    "\n",
    "dataset_Path_Full = dataset_Path + 'trainingTestingData.csv'\n",
    "DATA_PATH = dataset_Path_Full\n",
    "print(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "window_size = 6\n",
    "epoch_number = 100\n",
    "batch_number = 64\n",
    "fold_number = 6\n",
    "input_features = 8\n",
    "outputs_class = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>  Importing all the necessary libraries </h2></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 04:09:44.252855: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-03 04:09:44.334649: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-03 04:09:44.334666: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-08-03 04:09:44.676648: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-03 04:09:44.676690: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-03 04:09:44.676695: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for adaptive feature scaling\n",
    "class AdaptiveFeatureScaler:\n",
    "    def __init__(self):\n",
    "        self.feature_ranges = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Calculate the range (max - min) for each feature\n",
    "        self.feature_ranges = X.max(axis=0) - X.min(axis=0)\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Scale each feature based on the calculated range\n",
    "        if self.feature_ranges is None:\n",
    "            raise ValueError(\"Scaler not fitted. Call fit() first.\")\n",
    "        return X / self.feature_ranges\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess data\n",
    "def preprocessData(dataPath):\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "    columns = ['RSRP', 'RSRQ', 'Light', 'Mag', 'Acc', 'Sound', 'Proximity', 'Daytime', 'New_Recording', 'IO']\n",
    "    data = pd.DataFrame(data=df, columns=columns)\n",
    "\n",
    "    # Convert the string formatted data into float\n",
    "    data = data.astype('float')\n",
    "\n",
    "    # Replace standard scaling with AdaptiveFeatureScaler\n",
    "    afs = AdaptiveFeatureScaler()\n",
    "    data_scaled = afs.fit_transform(data[['RSRP', 'RSRQ', 'Light', 'Mag', 'Acc', 'Sound', 'Proximity', 'Daytime']])\n",
    "\n",
    "    # Combine the scaled features and the label\n",
    "    data_scaled['IO'] = data['IO']\n",
    "\n",
    "    # Continue with the rest of the data processing as before\n",
    "    states = data_scaled['IO'].value_counts().index\n",
    "    data_scaled.reset_index(drop=True, inplace=True)\n",
    "    data_scaled.index = data_scaled.index + 1\n",
    "    data_scaled.index.name = 'index'\n",
    "\n",
    "    return df, data_scaled\n",
    "\n",
    "# Function to balance data by selecting the same number of samples for each class\n",
    "def balanceData(df, data):\n",
    "    # Get the value counts of the 'IO' column\n",
    "    value_counts = df['IO'].value_counts()\n",
    "\n",
    "    # Find the minimum count of both labels\n",
    "    min_count = min(value_counts)\n",
    "\n",
    "    # Filter the DataFrame for 'Outdoor' and 'Indoor' categories\n",
    "    Outdoor = df[df['IO'] == 0].head(min_count).copy()\n",
    "    Indoor = df[df['IO'] == 1].head(min_count).copy()\n",
    "\n",
    "    balanced_data = pd.concat([Outdoor, Indoor], ignore_index=True)\n",
    "\n",
    "    return balanced_data\n",
    "\n",
    "# Function to encode the data labels using LabelEncoder\n",
    "def encodedData(balanced_data):\n",
    "    # Encoding the Data with suitable labels\n",
    "    label = LabelEncoder()\n",
    "    balanced_data['label'] = label.fit_transform(balanced_data['IO'])\n",
    "    return balanced_data\n",
    "\n",
    "# Function to standardize the features\n",
    "def standardizeData(encoded_data):\n",
    "    X = encoded_data[['RSRP', 'RSRQ', 'Light', 'Mag', 'Acc', 'Sound', 'Proximity', 'Daytime']]\n",
    "    y = encoded_data['label']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    scaled_X = pd.DataFrame(data=X, columns=['RSRP', 'RSRQ', 'Light', 'Mag', 'Acc', 'Sound', 'Proximity', 'Daytime'])\n",
    "    scaled_X['label'] = y.values\n",
    "\n",
    "    return scaled_X, X, y\n",
    "\n",
    "# Function to create overlapping frames from the data\n",
    "def get_frames(df, frame_size, hop_size, n_features=8):\n",
    "    N_FEATURES = n_features\n",
    "\n",
    "    frames = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - frame_size, hop_size):\n",
    "        # Get each feature for the current frame\n",
    "        features = [df[feature].values[i: i + frame_size] for feature in df.columns[:-1]]\n",
    "\n",
    "        # Retrieve the most often used label in this segment\n",
    "        label = stats.mode(df['label'][i: i + frame_size])[0][0]\n",
    "        frames.append(features)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Bring the segments into a better shape\n",
    "    frames = np.asarray(frames).reshape(-1, frame_size, N_FEATURES)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return frames, labels\n",
    "\n",
    "# Function to create overlapping frames from the standardized data\n",
    "def framedData(scaled_X, X, y, window_size=6, features_number=8):\n",
    "    frame_size = window_size\n",
    "    hop_size = int(frame_size / 2)\n",
    "    n_features = features_number\n",
    "\n",
    "    # Remove rows with NaN values and reset index\n",
    "    scaled_X = scaled_X.dropna().reset_index(drop=True)\n",
    "    X, y = get_frames(scaled_X, frame_size, hop_size, n_features)\n",
    "    return scaled_X, X, y\n",
    "\n",
    "# Function to split the data with cross-validation\n",
    "def splitDataWithCrossValidation(X, y, fold_number=6):\n",
    "    # Splitting the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "    # Creating the K-fold cross-validation iterator\n",
    "    kfold = StratifiedKFold(n_splits=fold_number, shuffle=True, random_state=0)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, kfold\n",
    "\n",
    "# Define the CNN-LSTM model with 2 parallel 1D CNN branches\n",
    "def parallel_CNN_LSTM(input_shape, n_outputs):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # First 1D CNN branch\n",
    "    x1 = Conv1D(64, kernel_size=3, activation='relu')(inputs)\n",
    "    x1 = MaxPooling1D(2)(x1)\n",
    "    x1 = LSTM(64)(x1)\n",
    "\n",
    "    # Second 1D CNN branch\n",
    "    x2 = Conv1D(64, kernel_size=5, activation='relu')(inputs)\n",
    "    x2 = MaxPooling1D(2)(x2)\n",
    "    x2 = LSTM(64)(x2)\n",
    "\n",
    "    # Concatenate the outputs from both branches\n",
    "    x = concatenate([x1, x2])\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(n_outputs, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Function to train the model using K-fold cross-validation\n",
    "def trainingModel(model_path, X, y, fold_number=6, epoch_number=100, batch_number=64):\n",
    "    # Create KFold instance\n",
    "    kfold = KFold(n_splits=fold_number, shuffle=True)\n",
    "\n",
    "    # Create empty lists to store the fold models and evaluation results\n",
    "    fold_models = []\n",
    "    fold_test_acc = []\n",
    "    fold_test_f1 = []\n",
    "    fold_prediction_times = []\n",
    "    fold_training_times = []\n",
    "\n",
    "    # Looping over the folds\n",
    "    for fold, (train_index, val_index) in enumerate(kfold.split(X, y)):\n",
    "        print(f\"Fold {fold+1}:\")\n",
    "\n",
    "        # Get the train and validation sets for this fold\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        # Define the filepath for the saved model specific to this fold\n",
    "        filepath = model_path + f\"bd_KIODNet_V{version}_W_{window_size}_F_{fold+1}.h5\"\n",
    "\n",
    "        # Define early stopping based on validation loss\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "        # Define a checkpoint to monitor the validation accuracy and save the best model\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "        input_shape = X_train[0].shape\n",
    "        # Create the CNN-LSTM model with parallel 1D CNNs\n",
    "        model = parallel_CNN_LSTM(input_shape, outputs_class)\n",
    "\n",
    "        # Compile the model\n",
    "        optimizer = Adam(learning_rate=0.001)\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # Start the training time measurement\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train the model with the checkpoint callback for this fold\n",
    "        history = model.fit(X_train_fold, y_train_fold, epochs=epoch_number, batch_size=batch_number,\n",
    "                            validation_data=(X_val_fold, y_val_fold), callbacks=[checkpoint, early_stop], verbose=1)\n",
    "\n",
    "        # End the training time measurement\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "\n",
    "        # Save the fold model to the list\n",
    "        fold_models.append(model)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_labels = np.argmax(test_pred, axis=1)\n",
    "\n",
    "        if len(y_test.shape) > 1 and y_test.shape[1] > 1:\n",
    "            test_true_labels = np.argmax(y_test, axis=1)\n",
    "        else:\n",
    "            test_true_labels = y_test\n",
    "\n",
    "        test_acc = accuracy_score(test_true_labels, test_pred_labels)\n",
    "        test_f1 = f1_score(test_true_labels, test_pred_labels, average='weighted')\n",
    "\n",
    "        # Calculate the prediction time\n",
    "        start_time = time.time()\n",
    "        model.predict(X_test[:1])\n",
    "        end_time = time.time()\n",
    "        prediction_time = (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "\n",
    "        # Save the evaluation results and times to the lists\n",
    "        fold_test_acc.append(test_acc)\n",
    "        fold_test_f1.append(test_f1)\n",
    "        fold_prediction_times.append(prediction_time)\n",
    "        fold_training_times.append(training_time)\n",
    "\n",
    "    # Calculate mean and median times for prediction and training\n",
    "    mean_prediction_time = np.mean(fold_prediction_times) / 1000  # Convert back to seconds\n",
    "    median_prediction_time = np.median(fold_prediction_times) / 1000  # Convert back to seconds\n",
    "    mean_training_time = np.mean(fold_training_times)\n",
    "    median_training_time = np.median(fold_training_times)\n",
    "\n",
    "    # Calculate the total number of parameters\n",
    "    total_params = model.count_params()\n",
    "\n",
    "    # Convert the total number of parameters to kilobytes (KB)\n",
    "    total_params_kb = total_params / 1024  # 1 KB = 1024 bytes\n",
    "\n",
    "    # Convert the total number of parameters to megabytes (MB)\n",
    "    total_memory_mb = total_params * 4 / (1024 * 1024)  # Assuming float32 precision (4 bytes per parameter)\n",
    "\n",
    "    # Save the averaged model\n",
    "    averaged_model = fold_models[0]\n",
    "    averaged_weights = averaged_model.get_weights()\n",
    "\n",
    "    n_splits = fold_number\n",
    "    # Loop over the layers of the models and average the weights\n",
    "    for layer in range(len(averaged_weights)):\n",
    "        for fold in range(1, n_splits):\n",
    "            averaged_weights[layer] += fold_models[fold].get_weights()[layer]\n",
    "\n",
    "        averaged_weights[layer] /= n_splits\n",
    "\n",
    "    # Set the averaged weights to the averaged model\n",
    "    averaged_model.set_weights(averaged_weights)\n",
    "    averaged_filepath = model_path + f\"bd_KIODNet_V{version}_W_{window_size}.h5\"\n",
    "    averaged_model.save(averaged_filepath)\n",
    "\n",
    "    return averaged_model, fold_test_acc, fold_test_f1, mean_prediction_time, median_prediction_time, mean_training_time, median_training_time, total_memory_mb, total_params_kb\n",
    "\n",
    "# Function to draw the confusion matrix and ROC curve\n",
    "def drawConfusionMatrix(myModel, X_test, y_test):\n",
    "    class_labels = ['Outdoor', 'Indoor']\n",
    "    # Measure the time it takes to predict a single sample\n",
    "    start_time = time.time()\n",
    "    predict_x = myModel.predict(X_test)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the prediction time\n",
    "    prediction_time = end_time - start_time\n",
    "    print('Prediction time:', prediction_time, 'seconds')\n",
    "\n",
    "    y_pred = np.argmax(predict_x, axis=1)\n",
    "    mat = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(mat, xticklabels=class_labels, yticklabels=class_labels, annot=True, linewidths=0.1, fmt='d', cmap='YlGnBu')\n",
    "    plt.title(\"Confusion matrix\", fontsize=15)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    # Calculate ROC curve and AUC\n",
    "    y_prob = predict_x[:, 1]  # Probability for the positive class (Indoor)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob, pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "df, data = preprocessData(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the data\n",
    "balanced_data = balanceData(df, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the data\n",
    "encoded_data = encodedData(balanced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaled_X, X, y = standardizeData(encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Framing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get framed data\n",
    "scaled_X, X, y = framedData(scaled_X, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation Datasets Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data with cross-validation\n",
    "X_train, X_test, y_train, y_test, kfold = splitDataWithCrossValidation(X, y, fold_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essemble Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and get the required outputs\n",
    "trained_model, test_acc, test_f1, mean_prediction_time, median_prediction_time, mean_training_time, median_training_time, total_memory_mb, total_params_kb = trainingModel(model_Path, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {test_acc}\")\n",
    "print(f\"F1-score: {test_f1}\")\n",
    "print(f\"Mean Prediction Time (sec): {mean_prediction_time:.6f}\")\n",
    "print(f\"Median Prediction Time (sec): {median_prediction_time:.6f}\")\n",
    "print(f\"Mean Training Time (sec): {mean_training_time:.6f}\")\n",
    "print(f\"Median Training Time (sec): {median_training_time:.6f}\")\n",
    "print(f\"Mean Single Prediction Time (msec): {mean_prediction_time * 1000:.3f}\")\n",
    "print(f\"Median Single Prediction Time (msec): {median_prediction_time * 1000:.3f}\")\n",
    "print(f\"Mean Required Memory (Mb): {total_memory_mb:.4f}\")\n",
    "print(f\"Median Required Memory (Mb): {total_memory_mb:.4f}\")\n",
    "print(f\"Mean Model Parameters (K): {total_params_kb:.2f}\")\n",
    "print(f\"Median Model Parameters (K): {total_params_kb:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix and ROC curve\n",
    "drawConfusionMatrix(trained_model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
